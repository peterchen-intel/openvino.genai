---
sidebar_position: 8
---

# Debug Logging

There are six log levels, which can be called explicitly or set via the `OPENVINO_LOG_LEVEL` environment variable: https://github.com/openvinotoolkit/openvino/blob/f35beb7f1355cb3f7f73b0d431933afbb6a8d3c6/src/inference/include/openvino/runtime/properties.hpp#L640

- -1 - `ov::log::Level::NO`
- 0 - `ov::log::Level::ERR`
- 1 - `ov::log::Level::WARNING`
- 2 - `ov::log::Level::INFO`
- 3 - `ov::log::Level::DEBUG`
- 4 - `ov::log::Level::TRACE`

When setting the environment variable `OPENVINO_LOG_LEVEL` > `ov::log::Level::DEBUG`, the properties of the compiled model will be printed.

<Tabs groupId="os">
    <TabItem label="Linux & macOS" value="linux_macos">
        ```sh
        export OPENVINO_LOG_LEVEL=3
        ```
    </TabItem>
    <TabItem label="Windows" value="windows">
        ```sh
        set OPENVINO_LOG_LEVEL=3
        ```
    </TabItem>
</Tabs>

After pipeline initialization and reading the model, the properties of the compiled model will be printed to the console.

```sh title="Output:"
NETWORK_NAME: Model0
OPTIMAL_NUMBER_OF_INFER_REQUESTS: 1
NUM_STREAMS: 1
INFERENCE_NUM_THREADS: 48
PERF_COUNT: NO
INFERENCE_PRECISION_HINT: bf16
PERFORMANCE_HINT: LATENCY
EXECUTION_MODE_HINT: PERFORMANCE
PERFORMANCE_HINT_NUM_REQUESTS: 0
ENABLE_CPU_PINNING: YES
SCHEDULING_CORE_TYPE: ANY_CORE
MODEL_DISTRIBUTION_POLICY:
ENABLE_HYPER_THREADING: NO
EXECUTION_DEVICES: CPU
CPU_DENORMALS_OPTIMIZATION: NO
LOG_LEVEL: LOG_NONE
CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE: 1
DYNAMIC_QUANTIZATION_GROUP_SIZE: 32
KV_CACHE_PRECISION: f16
AFFINITY: CORE
EXECUTION_DEVICES:
CPU: Intel(R) Xeon(R) Platinum 8468
```

When Speculative Decoding ot Prompt Lookup pipeline is executed, performance metrics will be also printed.

```sh title="Output:"
===============================
Total duration, sec: 26.6217
Draft model duration, sec: 1.60329
Main model duration, sec: 25.0184
Draft model duration, %: 6.02248
Main model duration, %: 93.9775
AVG acceptance rate, %: 21.6809
===============================
REQUEST_ID: 0
Main model iterations: 47
Token per sec: 3.75633
AVG acceptance rate, %: 21.6809
Accepted tokens by draft model: 51
Generated tokens: 100
Accepted token rate, %: 51
===============================
Request_id: 0 ||| 40 0 40 20 0 0 40 40 0 20 20 20 0 40 0 0 20 80 0 80 20 0 0 0 40 80 0 40 60 40 80 0 0 0 0 40 20 20 0 40 20 40 0 20 0 0 0
```

When a GGUF model is passed to the pipeline, the detailed debug info will also be printed.

```cpp
// Available in OV25.3+: enable_save_ov_model
pipe_config["enable_save_ov_model"] = true;

// Add in OV26.1: save_ov_model_quantize_mode
pipe_config["save_ov_model_quantize_mode"] = "GPU_OPTIMIZED";
```

`"ORIGINAL"` is also supported for `save_ov_model_quantize_mode`.

Recommend: save the optimized OV model to skip requant overhead in subsequent runs.

Note: requantization (de-quant then quant) adds extra load-time overhead when saving the OpenVINO model.
After the first run serializes OV models, OpenVINO GenAI `LLMPipeline` can directly reuse them
to run requantized models on NPU or achieve better performance on GPU in subsequent runs.

the output is similar to:

```sh title="Output:"
[GGUF Reader]: Save generated OpenVINO model to: <gguf_dir>/ov_model_gpu_optimized/openvino_tokenizer.xml done. Time: 8 ms
[GGUF Reader]: Save generated OpenVINO model to: <gguf_dir>/ov_model_gpu_optimized/openvino_detokenizer.xml done. Time: 1 ms
[GGUF Reader]: Loading and unpacking model from: <gguf_path>/<model_name>.gguf
[GGUF Reader]: OVModelQuantizeMode: quantization_mode=GPU_OPTIMIZED, save_file=YES
[GGUF Reader]: Loading and unpacking model done. Time: 8787ms
[GGUF Reader]: Start generating OpenVINO model...
[GGUF Reader]: Save generated OpenVINO model to: <gguf_dir>/ov_model_gpu_optimized/openvino_model.xml done. Time: 457 ms
[GGUF Reader]: Saved OpenVINO model to: <gguf_dir>/ov_model_gpu_optimized/openvino_model.xml
[GGUF Reader]: Model generation done. Time: 538ms
```

If `pipe_config["enable_save_ov_model"] = true` and `save_ov_model_quantize_mode` is omitted (default) or set to `ORIGINAL`,
the output is similar to:

```sh title="Output (ORIGINAL/default):"
[GGUF Reader]: Save generated OpenVINO model to: <gguf_dir>/ov_model_original/openvino_tokenizer.xml done. Time: 5 ms
[GGUF Reader]: Save generated OpenVINO model to: <gguf_dir>/ov_model_original/openvino_detokenizer.xml done. Time: 2 ms
[GGUF Reader]: Loading and unpacking model from: <gguf_path>/<model_name>.gguf
[GGUF Reader]: OVModelQuantizeMode: quantization_mode=ORIGINAL, save_file=YES
[GGUF Reader]: Loading and unpacking model done. Time: 789ms
[GGUF Reader]: Start generating OpenVINO model...
[GGUF Reader]: Save generated OpenVINO model to: <gguf_dir>/ov_model_original/openvino_model.xml done. Time: 448 ms
[GGUF Reader]: Saved OpenVINO model to: <gguf_dir>/ov_model_original/openvino_model.xml
[GGUF Reader]: Model generation done. Time: 796ms
```
